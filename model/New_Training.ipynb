{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0667c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, r2_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20423715",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.lepton = 0\n",
    "        self.nu = 0\n",
    "        self.probe_jet = 0\n",
    "        self.probe_jet_constituents = 0\n",
    "        self.balance_jets = 0\n",
    "        self.labels = 0\n",
    "        self.track_labels = 0\n",
    "    def __getitem__(self, idx):\n",
    "        return self.lepton[idx], self.nu[idx], self.probe_jet[idx], self.probe_jet_constituents[idx], self.balance_jets[idx], self.labels[idx], self.track_labels[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.lepton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f0a54fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag=\"U_1M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea68788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = torch.load(\"dataset\"+tag+\".pt\", weights_only=False)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dset, [0.8, 0.2])\n",
    "train_loader = DataLoader(train_dataset, batch_size=256)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ba4cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.pre_norm_Q = nn.LayerNorm(embed_dim)\n",
    "        self.pre_norm_K = nn.LayerNorm(embed_dim)\n",
    "        self.pre_norm_V = nn.LayerNorm(embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim,num_heads=num_heads,batch_first=True, dropout=0.25)\n",
    "        self.post_norm = nn.LayerNorm(embed_dim)\n",
    "        self.out = nn.Linear(embed_dim,embed_dim)\n",
    "    def forward(self, Query, Key, Value):\n",
    "        Query = self.pre_norm_Q(Query)\n",
    "        Key = self.pre_norm_K(Key)\n",
    "        Value = self.pre_norm_V(Value)\n",
    "        context, weights = self.attention(Query, Key, Value)\n",
    "        context = self.post_norm(context)\n",
    "        latent = Query + context\n",
    "        tmp = F.gelu(self.out(latent))\n",
    "        latent = latent + tmp\n",
    "        return_weights=False\n",
    "        if return_weights:\n",
    "            return latent,weights\n",
    "        return latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a6af8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):  \n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(Model, self).__init__()   \n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Initiliazer\n",
    "        self.lepton_initializer = nn.Linear(4, self.embed_dim)\n",
    "        self.MET_initializer = nn.Linear(2, self.embed_dim)\n",
    "        self.probe_jet_initializer = nn.Linear(3, self.embed_dim)\n",
    "        self.probe_jet_constituent_initializer = nn.Linear(4, self.embed_dim)\n",
    "        self.small_jet_initializer = nn.Linear(3, self.embed_dim)\n",
    "           \n",
    "        # Transformer Stack\n",
    "        self.event_encoder1 = Encoder(self.embed_dim, self.num_heads)\n",
    "        self.event_encoder2 = Encoder(self.embed_dim, self.num_heads)\n",
    "        self.event_encoder3 = Encoder(self.embed_dim, self.num_heads)\n",
    "        self.event_encoder4 = Encoder(self.embed_dim, self.num_heads)\n",
    "        self.event_encoder5 = Encoder(self.embed_dim, self.num_heads)\n",
    "        \n",
    "        # Kinematics Regression\n",
    "        self.top_regression_input = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.top_regression = nn.Linear(self.embed_dim, 4)\n",
    "        self.down_regression_input = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.down_regression = nn.Linear(self.embed_dim, 4)\n",
    "\n",
    "        # Direct Regression Task\n",
    "        self.direct_input = nn.Linear(8, self.embed_dim)\n",
    "        self.direct_regression = nn.Linear(self.embed_dim, 1)\n",
    "        \n",
    "        # Track Classification\n",
    "        self.track_classification = nn.Linear(self.embed_dim, 3)\n",
    "\n",
    "    def forward(self, lepton, MET, probe_jet, probe_jet_constituent, small_jet):\n",
    "        \n",
    "        # Feature initialization layers\n",
    "        lepton_embedding = torch.unsqueeze(F.gelu(self.lepton_initializer(lepton)), dim=1)\n",
    "        MET_embedding = torch.unsqueeze(F.gelu(self.MET_initializer(MET)), dim=1)\n",
    "        probe_jet_embedding = torch.unsqueeze(F.gelu(self.probe_jet_initializer(probe_jet)), dim=1)\n",
    "        probe_jet_constituent_embedding = F.gelu(self.probe_jet_constituent_initializer(probe_jet_constituent))\n",
    "        small_jet_embedding = F.gelu(self.small_jet_initializer(small_jet))\n",
    "        \n",
    "        num_leptons = lepton_embedding.shape[1]\n",
    "        num_MET = MET_embedding.shape[1]\n",
    "        num_probe_jet = probe_jet_embedding.shape[1]\n",
    "        num_constituents = probe_jet_constituent_embedding.shape[1]\n",
    "        num_small_jets = small_jet_embedding.shape[1]\n",
    "        \n",
    "        # Combine objects into single event tensor\n",
    "        event_embedding = torch.cat([probe_jet_embedding, probe_jet_constituent_embedding, lepton_embedding, MET_embedding, small_jet_embedding], axis=1)\n",
    "        \n",
    "        # Event Level Attention\n",
    "        event_embedding = self.event_encoder1(event_embedding,event_embedding,event_embedding)\n",
    "        event_embedding = self.event_encoder2(event_embedding,event_embedding,event_embedding)\n",
    "        event_embedding = self.event_encoder3(event_embedding,event_embedding,event_embedding)\n",
    "        #event_embedding = self.event_encoder4(event_embedding,event_embedding,event_embedding)\n",
    "        #event_embedding = self.event_encoder5(event_embedding,event_embedding,event_embedding)\n",
    "        \n",
    "        # Track Classificiation\n",
    "        start_idx=num_probe_jet\n",
    "        end_idx=start_idx+num_constituents\n",
    "        track_tensor = event_embedding[:,start_idx:end_idx]\n",
    "        track_output = self.track_classification(track_tensor)\n",
    "        \n",
    "        # Probe jet classification\n",
    "        start_idx=0\n",
    "        end_idx=num_probe_jet\n",
    "        probe_jet_tensor = torch.squeeze(event_embedding[:,start_idx:end_idx], dim=1)\n",
    "        \n",
    "        # Get Top output\n",
    "        top_kinematics = F.gelu(self.top_regression_input(probe_jet_tensor))\n",
    "        top_kinematics = self.top_regression(top_kinematics)\n",
    "        \n",
    "        # Get Down output\n",
    "        down_kinematics = F.gelu(self.down_regression_input(probe_jet_tensor))\n",
    "        down_kinematics = self.down_regression(down_kinematics)\n",
    "        down_kinematics = torch.cat([F.tanh(down_kinematics[:,0:3]), down_kinematics[:,3:]], axis=1)\n",
    "        \n",
    "        # Direct Regression output\n",
    "        direct_embedding = torch.cat([top_kinematics, down_kinematics], axis=1)\n",
    "        direct_embedding = F.gelu(self.direct_input(direct_embedding))\n",
    "        costheta = self.direct_regression(direct_embedding)\n",
    "        \n",
    "        # Construct output\n",
    "        output = torch.cat([top_kinematics, down_kinematics, costheta], axis=1)\n",
    "\n",
    "        return output, track_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b6ffcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available:  True\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU Available: \", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8deaa4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(32,4).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "MSE_loss_fn = nn.MSELoss()\n",
    "CCE_loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4a855ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters : 31148\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainable Parameters :\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "141fdac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.1239, -0.1371, -0.8665,  ..., -0.1440,  0.0027, -0.0483],\n",
      "        [-0.2162,  0.0592, -0.7414,  ...,  0.1410,  0.1185, -0.0281],\n",
      "        [-0.3144,  0.0642, -0.7334,  ...,  0.0894,  0.1028, -0.0337],\n",
      "        ...,\n",
      "        [-0.1431, -0.0591, -0.7414,  ..., -0.1682,  0.0924, -0.0438],\n",
      "        [-0.0967, -0.1011, -0.7617,  ..., -0.2614, -0.0172, -0.0228],\n",
      "        [-0.0734, -0.1476, -0.9021,  ..., -0.2058, -0.0434, -0.0404]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>), tensor([[[-2.1215,  0.7314, -0.4901],\n",
      "         [-2.2045,  0.6428, -0.5950],\n",
      "         [-2.2467,  0.8070, -0.5939],\n",
      "         ...,\n",
      "         [-2.1221,  0.5342, -0.3118],\n",
      "         [-2.1414,  0.4034, -0.2959],\n",
      "         [-2.1710,  0.4695, -0.3263]],\n",
      "\n",
      "        [[-1.7370,  0.8895,  0.2570],\n",
      "         [-1.8859,  0.9233,  0.2062],\n",
      "         [-1.8379,  0.8849,  0.2634],\n",
      "         ...,\n",
      "         [-1.5917,  0.6052,  0.4240],\n",
      "         [-1.6936,  0.5915,  0.3825],\n",
      "         [-1.6645,  0.6231,  0.4238]],\n",
      "\n",
      "        [[-2.0263,  0.7359,  0.1416],\n",
      "         [-2.0757,  0.8898,  0.1740],\n",
      "         [-1.9549,  0.7124,  0.1119],\n",
      "         ...,\n",
      "         [-1.3483,  0.6205,  0.7234],\n",
      "         [-1.4726,  0.6773,  0.6014],\n",
      "         [-1.3162,  0.4632,  0.6174]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.2993,  0.6306, -0.6329],\n",
      "         [-2.2463,  0.6998, -0.6023],\n",
      "         [-2.2378,  0.6104, -0.6829],\n",
      "         ...,\n",
      "         [-2.1494,  0.2353, -0.3381],\n",
      "         [-2.1794,  0.2466, -0.3160],\n",
      "         [-2.1519,  0.3358, -0.3284]],\n",
      "\n",
      "        [[-2.3999,  0.6615, -0.7059],\n",
      "         [-2.3030,  0.6687, -0.7288],\n",
      "         [-2.3052,  0.6041, -0.7523],\n",
      "         ...,\n",
      "         [-2.1862,  0.4272, -0.3663],\n",
      "         [-2.1428,  0.4184, -0.3734],\n",
      "         [-2.1319,  0.3600, -0.3932]],\n",
      "\n",
      "        [[-2.3041,  0.8228, -0.6432],\n",
      "         [-2.2965,  0.9092, -0.6263],\n",
      "         [-2.3231,  0.8912, -0.7083],\n",
      "         ...,\n",
      "         [-2.2036,  0.5783, -0.3754],\n",
      "         [-2.1860,  0.6023, -0.3499],\n",
      "         [-2.1275,  0.5965, -0.3494]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>))\n"
     ]
    }
   ],
   "source": [
    "for lepton, MET, probe_jet, constituents, small_jet, labels, track_labels in train_loader:\n",
    "    batch_size = len(lepton)\n",
    "    print(model(lepton.to(device), MET.to(device), probe_jet.to(device), constituents.to(device), small_jet.to(device)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5b8a212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, epochs=40):\n",
    "    \n",
    "    combined_history = []\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        cumulative_loss_train = 0\n",
    "        cumulative_loss_val = 0\n",
    "        \n",
    "        num_train = len(train_loader)*batch_size\n",
    "        num_val = len(val_loader)*batch_size\n",
    "\n",
    "        for lepton, MET, probe_jet, constituents, small_jet, labels, track_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output, trk_output = model(lepton.to(device), MET.to(device), probe_jet.to(device), constituents.to(device), small_jet.to(device))\n",
    "            \n",
    "            top_loss      = MSE_loss_fn(output[:,0:4], labels[:,0:4].to(device))\n",
    "            down_loss     = MSE_loss_fn(output[:,4:8], labels[:,4:8].to(device))\n",
    "            costheta_loss = MSE_loss_fn(output[:,-1], labels[:,-1].to(device))\n",
    "            track_loss    = CCE_loss_fn(trk_output, track_labels.to(device))\n",
    "            \n",
    "            alpha = 1\n",
    "            beta  = 1\n",
    "            gamma = 1\n",
    "            delta = 1\n",
    "            loss  = alpha*top_loss + beta*down_loss + gamma*costheta_loss + delta*track_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            cumulative_loss_train+=loss.detach().cpu().numpy().mean()\n",
    "            \n",
    "        cumulative_loss_train = cumulative_loss_train / num_train\n",
    "        \n",
    "        model.eval()\n",
    "        for lepton, MET, probe_jet, constituents, small_jet, labels, track_labels in val_loader:\n",
    "            output, trk_output = model(lepton.to(device), MET.to(device), probe_jet.to(device), constituents.to(device), small_jet.to(device))\n",
    "            \n",
    "            top_loss      = MSE_loss_fn(output[:,0:4], labels[:,0:4].to(device))\n",
    "            down_loss     = MSE_loss_fn(output[:,4:8], labels[:,4:8].to(device))\n",
    "            costheta_loss = MSE_loss_fn(output[:,-1], labels[:,-1].to(device))\n",
    "            track_loss    = CCE_loss_fn(trk_output, track_labels.to(device))\n",
    "            \n",
    "            loss  = alpha*top_loss + beta*down_loss + gamma*costheta_loss + delta*track_loss\n",
    "\n",
    "            cumulative_loss_val+=loss.detach().cpu().numpy().mean()\n",
    "        \n",
    "        cumulative_loss_val = cumulative_loss_val / num_val\n",
    "        \n",
    "        combined_history.append([cumulative_loss_train, cumulative_loss_val])\n",
    "\n",
    "        if e%1==0:\n",
    "            print('Epoch:',e+1,'\\tTrain Loss:',round(cumulative_loss_train,6),'\\tVal Loss:',round(cumulative_loss_val,6))\n",
    "            \n",
    "    return np.array(combined_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d97e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTrain Loss: 301.75235 \tVal Loss: 274.63794\n"
     ]
    }
   ],
   "source": [
    "history = train(model, optimizer, train_loader, test_loader, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aae7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history[20:,0], label=\"Train\")\n",
    "plt.plot(history[20:,1], label=\"Val\")\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94ceae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feats=9\n",
    "pred_labels = np.array([]).reshape(0,num_feats)\n",
    "true_labels = np.array([]).reshape(0,num_feats)\n",
    "for lepton, MET, probe_jet, constituents, small_jet, labels, track_labels in test_loader:\n",
    "    output, trk_output = model(lepton.to(device), MET.to(device), probe_jet.to(device), constituents.to(device), small_jet.to(device))\n",
    "    pred_labels = np.vstack((pred_labels,output.detach().cpu().numpy()))\n",
    "    true_labels = np.vstack((true_labels,labels.detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba6ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = ['top_px','top_py','top_pz','top_E','down_px','down_py','down_pz','down_E', 'costheta']\n",
    "ranges = [(-1000,1000),(-1000,1000),(-1000,1000),(0,1500),(-1.5,1.5),(-1.5,1.5),(-1.5,1.5),(0,150),(-1.5,1.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d99da3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Plotting predictions...\")\n",
    "for i in range(num_feats):\n",
    "    plt.figure()\n",
    "    plt.hist(np.ravel(true_labels[:,i]),histtype='step',color='r',label='True Distribution',bins=50,range=ranges[i])\n",
    "    plt.hist(np.ravel(pred_labels[:,i]),histtype='step',color='b',label='Predicted Distribution',bins=50,range=ranges[i])\n",
    "    plt.title(\"Predicted Ouput Distribution using Attention Model\")\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(feats[i],loc='right')\n",
    "    #plt.savefig(out_dir+\"/pred_1d_\"+feats[i]+\".png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Ouput Distribution using Attention Model\")\n",
    "    plt.hist2d(np.ravel(pred_labels[:,i]),np.ravel(true_labels[:,i]), bins=100,norm=mcolors.LogNorm(),range=(ranges[i],ranges[i]))\n",
    "    plt.xlabel('Predicted '+feats[i],loc='right')\n",
    "    plt.ylabel('True '+feats[i],loc='top')\n",
    "    diff = ranges[i][1] - ranges[i][0]\n",
    "    plt.text(ranges[i][1]-0.3*diff,ranges[i][0]+0.2*diff,\"$R^2$ value: \"+str(round(r2_score(np.ravel(true_labels[:,i]),np.ravel(pred_labels[:,i])),3)),backgroundcolor='r',color='k')\n",
    "    #print(\"R^2 value: \", round(r2_score(true_labels[:,i],predicted_labels[:,i]),3))\n",
    "    #plt.savefig(out_dir+\"/pred_2d_\"+feats[i]+\".png\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf727931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
